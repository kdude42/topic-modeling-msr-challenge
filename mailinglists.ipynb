{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983bbce5-c701-4a51-9960-0b1bf0690eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb3d82b-67c2-4283-a6d7-9842bbb23697",
   "metadata": {},
   "outputs": [],
   "source": [
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb=myclient['smartshark_2_1']\n",
    "mycol = mydb[\"project\"]\n",
    "\n",
    "projects=[]\n",
    "list2=[]\n",
    "list3=[]\n",
    "list4=[]\n",
    "detailedissues=[]\n",
    "joinedissues = mydb.project.aggregate([{\"$lookup\":{\n",
    "            \"from\": \"mailing_list\",       # other table name\n",
    "            \"localField\": \"_id\",        # key field in collection 2\n",
    "            \"foreignField\": \"project_id\",      # key field in collection 1\n",
    "            \"as\": \"linked_collections\"   # alias for resulting table\n",
    "        }},\n",
    "        {   \"$unwind\":\"$linked_collections\"},\n",
    "                                \n",
    "         {\"$lookup\":{\n",
    "            \"from\": \"message\",       # other table name\n",
    "            \"localField\": \"linked_collections._id\",        # key field in collection 2\n",
    "            \"foreignField\": \"mailing_list_id\",      # key field in collection 1\n",
    "            \"as\": \"linked_collections2\"   # alias for resulting table\n",
    "        }},\n",
    "       \n",
    "         {   \"$unwind\":\"$linked_collections2\" },                             \n",
    "        {\"$project\": {\"_id\":1,\"name\":1,\"linked_collections2.body\":1}}\n",
    "        ])\n",
    "\n",
    "\n",
    "listofmessages=[]\n",
    "i=6\n",
    "while i>1:\n",
    "\n",
    " for docs in joinedissues:\n",
    "      list3.append((docs[\"_id\"],docs[\"name\"],docs[\"linked_collections2\"]))\n",
    "      i=i-1\n",
    "\n",
    "for project in mycol.find():\n",
    " projects.append(project[\"_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339151fd-9c7e-46ae-9414-ce64fd0fcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectmail=[]\n",
    "for projectid in projects:\n",
    "    detailedmessage=[]\n",
    "  \n",
    "\n",
    "    hit1=0\n",
    "    for item,name,comment in list3:\n",
    "        \n",
    "      \n",
    "        if projectid==item:\n",
    "            hit1=hit1+1\n",
    "            detailedmessage.append(comment)\n",
    "    projectmail.append(detailedmessage)\n",
    "\n",
    "    listofmessages.append(hit1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1288d-d88d-4155-8817-75e79f4d2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import string\n",
    "from sklearn.feature_extraction import text\n",
    "# from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# all_names = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ignore = ['{','}','.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\",'\\n','\\n2','ok','yes','no','thanks','good','sure','did','Thanks','Sure','welcome']  \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(ignore)\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_words, max_features=500)\n",
    "\n",
    "cleaned=[]\n",
    "def letters_only(astr):\n",
    " return astr.isalpha()\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ' or pos[:2]=='VB'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)\n",
    "def isoneword(text):\n",
    "    if len(text)>2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "for test in projectmail:\n",
    "    for post in test:\n",
    "      values=str(post.get('body'))\n",
    "      sentence=nouns_adj(values)\n",
    "      cleaned.append(' '.join([lemmatizer.lemmatize(word) for word in sentence.split() if  letters_only(word) and word not in stop_words and word not in ignore and word not in exclude]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e94dd8-4c11-417c-9f91-68d5010911c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=100, random_state=42).fit(transformed)\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    " label = '{}: '.format(topic_idx)\n",
    " print(label, \" \".join([cv.get_feature_names_out()[i]\n",
    " for i in topic.argsort()[:-9:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824cfa00-3121-4100-9903-12d8fd5c78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "test=projectmail[0]\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "subresults=[]\n",
    "polresults=[]\n",
    "for test in projectmail[0]\n",
    " for post in test:\n",
    "     values=str(post.get('body'))\n",
    "     \n",
    "     polarity=TextBlob(values).sentiment.subjectivity\n",
    "     subjectivity=TextBlob(values).sentiment.polarity\n",
    "     subresults.append(subjectivity)\n",
    "     polresults.append(polarity)\n",
    "\n",
    "one=np.asarray(subresults)\n",
    "two=np.asarray(polresults)\n",
    "print(np.mean(one,axis=None))\n",
    "print(np.mean(two,axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef300cec-2d70-4a83-ae00-e72d584e4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gensim\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import pprint\n",
    "final=[doc.split() for doc in cleaned]\n",
    "dictionary = gensim.corpora.Dictionary(final)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.4, keep_n=120000)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in final]\n",
    "\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=0,\n",
    "                                           \n",
    "                                           passes=40,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "lda_model.print_topics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
